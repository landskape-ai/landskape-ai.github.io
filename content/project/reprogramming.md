+++
date = "2022-03-03"
title = "Forgetting exempt continual learning with a dictionary of reprogrammers"
description = "Developing a new heterogeneous multi-modal continual learning scenario with mitigation of catastrophic forgetting using a dictionary of task specific reprogrammers coupled with a Foundation Model."
short_description = ""
project_id = "reprogrammer"
picture = "projects/robust.jpg"
external_link = ""
participants_block_position = "down"
include_participants_portraits = true
sort_position = 2

[[participants]]
    name = "Diganta Misra"
    id = "diganta"

[[participants]]
    name = "Bharat Runwal"
    id = "Bharat"

+++

**Ongoing Project**

## Abstract

In this work, we formalize a novel approach of addressing catastrophic forgetting in deep neural networks trained in a continual learning regime over a sequence of $n$ tasks. We propose using a dictionary of reprogrammers coupled with a pretrained Foundation model which we term as "Continual Reprogramming". Furthermore, we also formalize a novel heteregenous multi-modal continual learning scenario where the tasks are not only different in nature but also in the modalities of data they are trained on. We demonstrate the efficacy of our approach on a variety of datasets and show that our approach outperforms the state-of-the-art methods in the continual learning regime.

## References:

[1] Elsayed, Gamaleldin F., Ian Goodfellow, and Jascha Sohl-Dickstein. "Adversarial reprogramming of neural networks." arXiv preprint arXiv:1806.11146 (2018).