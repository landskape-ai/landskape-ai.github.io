<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Landskape AI</title>
    <link>https://landskape.ai/project/</link>
    <description>Recent content in Projects on Landskape AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; Landskape AI, 2020 </copyright>
    <lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://landskape.ai/project/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>APP: Anytime Progressive Pruning</title>
      <link>https://landskape.ai/project/lth/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://landskape.ai/project/lth/</guid>
      <description>Ongoing Project
Abstract With the latest advancements in deep learning, there has been a lot of focus on the online learning paradigm due to its relevance in industrial and practical settings. While many methods have been investigated for optimal learning settings in scenarios where the data stream is continuous over time, training sparse networks in such settings have often been overlooked. In this paper, we explore the problem statement of training a neural network with a target sparsity in a subbranch of online learning, specifically the anytime learning paradigm, and propose a novel way of progressive pruning that we term Anytime Progressive Pruning(APP), which strongly outperforms the baseline dense and Anytime OSP models across multiple architectures and datasets under short, moderate, and long sequence training.</description>
    </item>
    
  </channel>
</rss>
