<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Landskape AI</title>
    <link>https://landskape.ai/project/</link>
    <description>Recent content in Projects on Landskape AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; Landskape AI, 2020 </copyright>
    <lastBuildDate>Thu, 03 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://landskape.ai/project/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AP/GP: Anytime Progressive Growing &#43; Pruning</title>
      <link>https://landskape.ai/project/apgp/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://landskape.ai/project/apgp/</guid>
      <description>Ongoing Project
Abstract In this work, we propose effective and efficient solutions in regards to the issues presented in Misra et al. (2022) &amp;ldquo;APP: Anytime Progressive Pruning&amp;rdquo;. Precisely, we propose a new algorithm &amp;ldquo;Anytime Progressive Pruning + Growing&amp;rdquo; (AP/GP) that solves the issue of APP with no replay and high target sparsity while inducing similar regularization effect as that of APP. The proposed algorithm takes advantage of progressive growing and pruning techniques to achieve the same effective target sparsity as that of APP while not requiring the prior knowledge of the total number of megabatches in the stream.</description>
    </item>
    
    <item>
      <title>Reprogrammers are robust learners</title>
      <link>https://landskape.ai/project/reprogramming/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://landskape.ai/project/reprogramming/</guid>
      <description>Ongoing Project
Abstract In this work, we investigate the effect of adversarial reprogramming on the robustness of the model upon transfer. We further are interested in adding a robustness objective into the constrained optimization process of reprogramming. We further demonstrate the effect of scaling on reprogramming and do an in-depth cost analysis breakdown comparison of reprogramming and fine-tuning.
References: [1] Elsayed, Gamaleldin F., Ian Goodfellow, and Jascha Sohl-Dickstein. &amp;ldquo;Adversarial reprogramming of neural networks.</description>
    </item>
    
  </channel>
</rss>
